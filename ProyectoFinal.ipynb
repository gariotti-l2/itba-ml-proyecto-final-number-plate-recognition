{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de Patentes con Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "Un proyecto para reconocer los caracteres de una chapa patente basado en el documento de Mathew Earl https://matthewearl.github.io/2016/05/06/cnn-anpr/ que surge de un paper de los ingenieros de google https://arxiv.org/pdf/1312.6082v4.pdf con respecto a la utilización de machine learning y las imagenes de street view para obtener una red que permita resolver los captcha de Google.\n",
    "Siguiendo la lógica del paper Mathew propone generar el dataset de patentes de manera sintética y resolver con la misma red del paper la identificación de los caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entradas, salidas y windowing\n",
    "Para minimizar los requisitos computacionales, la red operara con imágenes de entrada en escala de grises de 128x64.\n",
    "Con esa resolución todavía sigue siendo legible la patente.\n",
    "Para detectar matrículas en imágenes más grandes, se utilizan varias escalas con el metodo de sliding window.\n",
    "\n",
    "Para cada entrada de 128x64 la red tiene una salida:\n",
    "* La probabilidad de que la patente esté presente en la imagen de entrada. \n",
    "* La probabilidad del caracter en cada posición, es decir. para cada una de las 7 posiciones posibles debería devolver una distribución de probabilidad entre los caracteres posibles.\n",
    "\n",
    "**Esta red solo sirve para patentes de automotores y del formato mercosur.\n",
    "\n",
    "\n",
    "Una patente está presente si y solo si:\n",
    "* La patente está contenida totalmente dentro de los límites de la imagen.\n",
    "* El ancho de la patente es inferior al 80% del ancho de la imagen, y la altura de la patente es inferior al 87.5% de la altura de la imagen.\n",
    "* El ancho de la patente es mayor que el 60% del ancho de la imagen o la altura de la patente es mayor que el 60% de la altura de la imagen.\n",
    "\n",
    "Con estos números podemos usar una ventana deslizante que se mueve de 8 píxeles a la vez, y hace zoom en raiz de 2 veces entre niveles de zoom. Cualquier duplicado que ocurra se combina en un paso de procesamiento posterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando el dataset\n",
    "\n",
    "Se genera un conjunto de imágenes de 128x64 junto con la salida esperada. \n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "<img src=\"procesogeneracion.png\">\n",
    "\n",
    "Indicando en la primera parte los caracteres de la patente y luego el digito si se encuentra presente completa o no.\n",
    "\n",
    "El texto y el color de la imagen se eligen al azar, pero el texto debe ser una cierta cantidad más oscura que la placa. Esto es para simular la variación de iluminación del mundo real. El ruido se agrega al final, no solo para tener en cuenta el ruido real del sensor, sino también para evitar que la red dependa demasiado de los bordes bien definidos como se vería con una imagen de entrada fuera de foco.\n",
    "\n",
    "Tener un fondo es importante ya que significa que la red debe aprender a identificar los límites de la placa de matrícula sin \"trampa\": si se usara un fondo negro, por ejemplo, la red puede aprender a identificar la ubicación de la placa en función de la no oscuridad, lo que claramente No funciona con imágenes reales de coches.\n",
    "\n",
    "**Los fondos provienen de la base de datos de SUN, que contiene más de 100,000 imágenes. Es importante que la cantidad de imágenes sea grande para evitar que la red \"memorice\" imágenes de fondo.\n",
    "\n",
    "La fuente es la que se usa en Argentina para las patentes del Mercosur, según el documento de la DNRA.\n",
    "\n",
    "La transformación aplicada a la patente (y su máscara) es una transformación afín basada en un giro aleatorio, inclinación, desvío, traslación y escala. \n",
    "\n",
    "<img src=\"datagenerada.png\">\n",
    "\n",
    "#### mygenerator.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate training and test images.\n",
    "https://github.com/matthewearl/deep-anpr\n",
    "https://github.com/sapphirelin/re-deep-anpr\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "__all__ = (\n",
    "    'generate_ims',\n",
    ")\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "\n",
    "import common\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FONT_DIR = \"./fonts\"\n",
    "FONT_HEIGHT = 32  # Pixel size to which the chars are resized\n",
    "\n",
    "OUTPUT_SHAPE = (64, 128)\n",
    "\n",
    "CHARS = common.CHARS + \" \"\n",
    "\n",
    "def make_char_ims(font_path, output_height):\n",
    "    font_size = output_height * 4\n",
    "\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "    height = max(font.getsize(c)[1] for c in CHARS)\n",
    "\n",
    "    for c in CHARS:\n",
    "        width = font.getsize(c)[0]\n",
    "        im = Image.new(\"RGBA\", (width, height), (0, 0, 0))\n",
    "\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        draw.text((0, 0), c, (255, 255, 255), font=font)\n",
    "        scale = float(output_height) / height\n",
    "        im = im.resize((int(width * scale), output_height), Image.ANTIALIAS)\n",
    "        yield c, numpy.array(im)[:, :, 0].astype(numpy.float32) / 255.\n",
    "\n",
    "def euler_to_mat(yaw, pitch, roll):\n",
    "    # Rotate clockwise about the Y-axis\n",
    "    c, s = math.cos(yaw), math.sin(yaw)\n",
    "    M = numpy.matrix([[c, 0., s],\n",
    "                      [0., 1., 0.],\n",
    "                      [-s, 0., c]])\n",
    "    # Rotate clockwise about the X-axis\n",
    "    c, s = math.cos(pitch), math.sin(pitch)\n",
    "    M = numpy.matrix([[1., 0., 0.],\n",
    "                      [0., c, -s],\n",
    "                      [0., s, c]]) * M\n",
    "    # Rotate clockwise about the Z-axis\n",
    "    c, s = math.cos(roll), math.sin(roll)\n",
    "    M = numpy.matrix([[c, -s, 0.],\n",
    "                      [s, c, 0.],\n",
    "                      [0., 0., 1.]]) * M\n",
    "    return M\n",
    "\n",
    "def pick_colors():\n",
    "    first = True\n",
    "    while first or plate_color - text_color < 0.3:\n",
    "        text_color = random.random()\n",
    "        plate_color = random.random()\n",
    "        if text_color > plate_color:\n",
    "            text_color, plate_color = plate_color, text_color\n",
    "        first = False\n",
    "    return text_color, plate_color\n",
    "\n",
    "def make_affine_transform(from_shape, to_shape,\n",
    "                          min_scale, max_scale,\n",
    "                          scale_variation=1.0,\n",
    "                          rotation_variation=1.0,\n",
    "                          translation_variation=1.0):\n",
    "    out_of_bounds = False\n",
    "\n",
    "    from_size = numpy.array([[from_shape[1], from_shape[0]]]).T\n",
    "    to_size = numpy.array([[to_shape[1], to_shape[0]]]).T\n",
    "\n",
    "    scale = random.uniform((min_scale + max_scale) * 0.5 -\n",
    "                           (max_scale - min_scale) * 0.5 * scale_variation,\n",
    "                           (min_scale + max_scale) * 0.5 +\n",
    "                           (max_scale - min_scale) * 0.5 * scale_variation)\n",
    "    if scale > max_scale or scale < min_scale:\n",
    "        out_of_bounds = True\n",
    "    roll = random.uniform(-0.3, 0.3) * rotation_variation\n",
    "    pitch = random.uniform(-0.2, 0.2) * rotation_variation\n",
    "    yaw = random.uniform(-1.2, 1.2) * rotation_variation\n",
    "\n",
    "    # Compute a bounding box on the skewed input image (`from_shape`).\n",
    "    M = euler_to_mat(yaw, pitch, roll)[:2, :2]\n",
    "    h, w = from_shape\n",
    "    corners = numpy.matrix([[-w, +w, -w, +w],\n",
    "                            [-h, -h, +h, +h]]) * 0.5\n",
    "    skewed_size = numpy.array(numpy.max(M * corners, axis=1) -\n",
    "                              numpy.min(M * corners, axis=1))\n",
    "\n",
    "    # Set the scale as large as possible such that the skewed and scaled shape\n",
    "    # is less than or equal to the desired ratio in either dimension.\n",
    "    scale *= numpy.min(to_size / skewed_size)\n",
    "\n",
    "    # Set the translation such that the skewed and scaled image falls within\n",
    "    # the output shape's bounds.\n",
    "    trans = (numpy.random.random((2, 1)) - 0.5) * translation_variation\n",
    "    trans = ((2.0 * trans) ** 5.0) / 2.0\n",
    "    if numpy.any(trans < -0.5) or numpy.any(trans > 0.5):\n",
    "        out_of_bounds = True\n",
    "    trans = (to_size - skewed_size * scale) * trans\n",
    "\n",
    "    center_to = to_size / 2.\n",
    "    center_from = from_size / 2.\n",
    "\n",
    "    M = euler_to_mat(yaw, pitch, roll)[:2, :2]\n",
    "    M *= scale\n",
    "    M = numpy.hstack([M, trans + center_to - M * center_from])\n",
    "\n",
    "    return M, out_of_bounds\n",
    "\n",
    "\n",
    "def generate_code(): \n",
    "    return \"{}{} {}{}{} {}{}\".format(   \n",
    "        random.choice(common.LETTERS),\n",
    "        random.choice(common.LETTERS),\n",
    "        random.choice(common.DIGITS),\n",
    "        random.choice(common.DIGITS),\n",
    "        random.choice(common.DIGITS),\n",
    "        random.choice(common.LETTERS),\n",
    "        random.choice(common.LETTERS))\n",
    "\n",
    "\n",
    "def rounded_rect(shape, radius):\n",
    "    out = numpy.ones(shape)\n",
    "    out[:radius, :radius] = 0.0\n",
    "    out[-radius:, :radius] = 0.0\n",
    "    out[:radius, -radius:] = 0.0\n",
    "    out[-radius:, -radius:] = 0.0\n",
    "\n",
    "    cv2.circle(out, (radius, radius), radius, 1.0, -1)\n",
    "    cv2.circle(out, (radius, shape[0] - radius), radius, 1.0, -1)\n",
    "    cv2.circle(out, (shape[1] - radius, radius), radius, 1.0, -1)\n",
    "    cv2.circle(out, (shape[1] - radius, shape[0] - radius), radius, 1.0, -1)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_plate(font_height, char_ims):\n",
    "    h_padding = random.uniform(0.2, 0.4) * font_height\n",
    "    v_padding = random.uniform(0.1, 0.3) * font_height\n",
    "    spacing = font_height * random.uniform(-0.05, 0.05)\n",
    "    radius = 1 + int(font_height * 0.1 * random.random())\n",
    "\n",
    "    code = generate_code()\n",
    "    text_width = sum(char_ims[c].shape[1] for c in code)\n",
    "    text_width += (len(code) - 1) * spacing\n",
    "\n",
    "    out_shape = (int(font_height + v_padding * 2),\n",
    "                 int(text_width + h_padding * 2))\n",
    "\n",
    "    text_color, plate_color = pick_colors()\n",
    "\n",
    "    text_mask = numpy.zeros(out_shape)\n",
    "\n",
    "    x = h_padding\n",
    "    y = v_padding\n",
    "    for c in code:\n",
    "        char_im = char_ims[c]\n",
    "        ix, iy = int(x), int(y)\n",
    "        text_mask[iy:iy + char_im.shape[0], ix:ix + char_im.shape[1]] = char_im\n",
    "        x += char_im.shape[1] + spacing\n",
    "\n",
    "    plate = (numpy.ones(out_shape) * plate_color * (1. - text_mask) +\n",
    "             numpy.ones(out_shape) * text_color * text_mask)\n",
    "\n",
    "    return plate, rounded_rect(out_shape, radius), code.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def generate_bg(num_bg_images):\n",
    "    found = False\n",
    "    while not found:\n",
    "        fname = \"bgs/{:08d}.jpg\".format(random.randint(0, num_bg_images - 1))\n",
    "        bg = cv2.imread(fname, 0) / 255.\n",
    "        if (bg.shape[1] >= OUTPUT_SHAPE[1] and\n",
    "                bg.shape[0] >= OUTPUT_SHAPE[0]):\n",
    "            found = True\n",
    "\n",
    "    x = random.randint(0, bg.shape[1] - OUTPUT_SHAPE[1])\n",
    "    y = random.randint(0, bg.shape[0] - OUTPUT_SHAPE[0])\n",
    "    bg = bg[y:y + OUTPUT_SHAPE[0], x:x + OUTPUT_SHAPE[1]]\n",
    "\n",
    "    return bg\n",
    "\n",
    "\n",
    "def generate_im(char_ims, num_bg_images):\n",
    "    bg = generate_bg(num_bg_images)\n",
    "\n",
    "    plate, plate_mask, code = generate_plate(FONT_HEIGHT, char_ims)\n",
    "    \n",
    "    M, out_of_bounds = make_affine_transform(\n",
    "        from_shape=plate.shape,\n",
    "        to_shape=bg.shape,\n",
    "        min_scale=0.6,\n",
    "        max_scale=0.875,\n",
    "        rotation_variation=1.0,\n",
    "        scale_variation=1.5,\n",
    "        translation_variation=1.2)\n",
    "    plate = cv2.warpAffine(plate, M, (bg.shape[1], bg.shape[0]))\n",
    "    plate_mask = cv2.warpAffine(plate_mask, M, (bg.shape[1], bg.shape[0]))\n",
    "\n",
    "    out = plate * plate_mask + bg*(1.0 - plate_mask)\n",
    "\n",
    "    out = cv2.resize(out, (OUTPUT_SHAPE[1], OUTPUT_SHAPE[0]))\n",
    "\n",
    "    out += numpy.random.normal(scale=0.05, size=out.shape)\n",
    "    out = numpy.clip(out, 0., 1.)\n",
    "\n",
    "    return out, code, not out_of_bounds\n",
    "\n",
    "\n",
    "def load_fonts(folder_path):\n",
    "    font_char_ims = {}\n",
    "    fonts = [f for f in os.listdir(folder_path) if f.endswith('.ttf')]\n",
    "    for font in fonts:\n",
    "        font_char_ims[font] = dict(make_char_ims(os.path.join(folder_path,\n",
    "                                                              font),\n",
    "                                                 FONT_HEIGHT))\n",
    "    return fonts, font_char_ims\n",
    "\n",
    "\n",
    "def generate_ims():\n",
    "    \"\"\"\n",
    "    Generate number plate images.\n",
    "    :return:\n",
    "        Iterable of number plate images.\n",
    "    \"\"\"\n",
    "    variation = 1.0\n",
    "    fonts, font_char_ims = load_fonts(FONT_DIR)\n",
    "    num_bg_images = len(os.listdir(\"bgs\"))\n",
    "    while True:\n",
    "        yield generate_im(font_char_ims[random.choice(fonts)], num_bg_images)\n",
    "\n",
    "\n",
    "generate_amount = 25000  #MUST ESPECIFY THE AMOUNT \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if os.path.isdir(\"test\"): os.rmdir('test')\n",
    "    os.mkdir(\"test\")\n",
    "    im_gen = itertools.islice(generate_ims(), generate_amount)\n",
    "    for img_idx, (im, c, p) in enumerate(im_gen):\n",
    "        fname = \"test/{:08d}_{}_{}.png\".format(img_idx, c,\n",
    "                                               \"1\" if p else \"0\")\n",
    "        print(fname)\n",
    "        cv2.imwrite(fname, im * 255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La red\n",
    "\n",
    "<img src=\"lared.png\">\n",
    "\n",
    "\n",
    "La capa de salida tiene un nodo (que se muestra a la izquierda) que se utiliza como indicador de presencia. El resto codifica la probabilidad de una placa de matrícula en particular: cada columna, como se muestra en el diagrama, corresponde con uno de los dígitos en la placa de matrícula, y cada nodo da la probabilidad de que el carácter correspondiente esté presente. \n",
    "\n",
    "Todas las capas de salida utilizan la función de activación ReLU. El nodo de presencia tiene una sigmoidea como se usa normalmente para salidas binarias. Los otros nodos de salida utilizan una softmax entre los caracteres (es decir, de modo que la probabilidad en cada columna se suma a una).\n",
    "\n",
    "La función de loss se define en términos de la entropía cruzada entre la etiqueta y la salida de la red. Para la estabilidad numérica, las funciones de activación de la capa final se incorporan al cálculo de entropía cruzada utilizando softmax_cross_entropy_with_logits y sigmoid_cross_entropy_with_logits. \n",
    "\n",
    "El entrenamiento lo hicimos en una maquina sin placa gráfica durante 8 días, luego se modificaron algunos parametros varias veces y los últimos pesos fueron generados con un entrenamiento de 6 horas usando una maquina con gpu en google cloud.\n",
    "\n",
    "#### model.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "__all__ = (\n",
    "    'get_training_model',\n",
    "    'get_detect_model',\n",
    "    'WINDOW_SHAPE',\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import common\n",
    "\n",
    "WINDOW_SHAPE = (64, 128)\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W, stride=(1, 1), padding='SAME'):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride[0], stride[1], 1],\n",
    "                        padding=padding)\n",
    "\n",
    "\n",
    "def max_pool(x, ksize=(2, 2), stride=(2, 2)):\n",
    "    return tf.nn.max_pool(x, ksize=[1, ksize[0], ksize[1], 1],\n",
    "                          strides=[1, stride[0], stride[1], 1], padding='SAME')\n",
    "\n",
    "\n",
    "def avg_pool(x, ksize=(2, 2), stride=(2, 2)):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, ksize[0], ksize[1], 1],\n",
    "                          strides=[1, stride[0], stride[1], 1], padding='SAME')\n",
    "\n",
    "def convolutional_layers():\n",
    "    \"\"\"\n",
    "    Get the convolutional layers of the model.\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(tf.float32, [None, None, None])\n",
    "\n",
    "    # First layer\n",
    "    W_conv1 = weight_variable([5, 5, 1, 48])\n",
    "    b_conv1 = bias_variable([48])\n",
    "    x_expanded = tf.expand_dims(x, 3)\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_expanded, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool(h_conv1, ksize=(2, 2), stride=(2, 2))\n",
    "\n",
    "    # Second layer\n",
    "    W_conv2 = weight_variable([5, 5, 48, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool(h_conv2, ksize=(2, 1), stride=(2, 1))\n",
    "\n",
    "    # Third layer\n",
    "    W_conv3 = weight_variable([5, 5, 64, 128])\n",
    "    b_conv3 = bias_variable([128])\n",
    "\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    h_pool3 = max_pool(h_conv3, ksize=(2, 2), stride=(2, 2))\n",
    "\n",
    "    return x, h_pool3, [W_conv1, b_conv1,\n",
    "                        W_conv2, b_conv2,\n",
    "                        W_conv3, b_conv3]\n",
    "\n",
    "\n",
    "def get_training_model():\n",
    "    \"\"\"\n",
    "    The training model acts on a batch of 128x64 windows, and outputs a (1 +\n",
    "    7 * len(common.CHARS) vector, `v`. `v[0]` is the probability that a plate is\n",
    "    fully within the image and is at the correct scale.\n",
    "\n",
    "    `v[1 + i * len(common.CHARS) + c]` is the probability that the `i`'th\n",
    "    character is `c`.\n",
    "    \"\"\"\n",
    "    x, conv_layer, conv_vars = convolutional_layers()\n",
    "\n",
    "    # Densely connected layer\n",
    "    W_fc1 = weight_variable([32 * 8 * 128, 2048])\n",
    "    b_fc1 = bias_variable([2048])\n",
    "\n",
    "    conv_layer_flat = tf.reshape(conv_layer, [-1, 32 * 8 * 128])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(conv_layer_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Output layer\n",
    "    W_fc2 = weight_variable([2048, 1 + 7 * len(common.CHARS)])\n",
    "    b_fc2 = bias_variable([1 + 7 * len(common.CHARS)])\n",
    "\n",
    "    y = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "    return (x, y, conv_vars + [W_fc1, b_fc1, W_fc2, b_fc2])\n",
    "\n",
    "\n",
    "def get_detect_model():\n",
    "    \"\"\"\n",
    "    The same as the training model, except it acts on an arbitrarily sized\n",
    "    input, and slides the 128x64 window across the image in 8x8 strides.\n",
    "    The output is of the form `v`, where `v[i, j]` is equivalent to the output\n",
    "    of the training model, for the window at coordinates `(8 * i, 4 * j)`.\n",
    "    \"\"\"\n",
    "    x, conv_layer, conv_vars = convolutional_layers()\n",
    "\n",
    "    # Fourth layer\n",
    "    W_fc1 = weight_variable([8 * 32 * 128, 2048])\n",
    "    W_conv1 = tf.reshape(W_fc1, [8, 32, 128, 2048])\n",
    "    b_fc1 = bias_variable([2048])\n",
    "    h_conv1 = tf.nn.relu(conv2d(conv_layer, W_conv1,\n",
    "                                stride=(1, 1), padding=\"VALID\") + b_fc1)\n",
    "    # Fifth layer\n",
    "    W_fc2 = weight_variable([2048, 1 + 7 * len(common.CHARS)])\n",
    "    W_conv2 = tf.reshape(W_fc2, [1, 1, 2048, 1 + 7 * len(common.CHARS)])\n",
    "    b_fc2 = bias_variable([1 + 7 * len(common.CHARS)])\n",
    "    h_conv2 = conv2d(h_conv1, W_conv2) + b_fc2\n",
    "\n",
    "    return (x, h_conv2, conv_vars + [W_fc1, b_fc1, W_fc2, b_fc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: train.py\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import functools\n",
    "import glob\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "import common\n",
    "import mygenerator\n",
    "import model\n",
    "\n",
    "\n",
    "def code_to_vec(p, code):\n",
    "    def char_to_vec(c):\n",
    "        y = numpy.zeros((len(common.CHARS),))\n",
    "        y[common.CHARS.index(c)] = 1.0\n",
    "        return y\n",
    "\n",
    "    c = numpy.vstack([char_to_vec(c) for c in code])\n",
    "\n",
    "    return numpy.concatenate([[1. if p else 0], c.flatten()])\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def read_data(img_glob):\n",
    "    for fname in sorted(glob.glob(img_glob)):\n",
    "        im = cv2.imread(fname)[:, :, 0].astype(numpy.float32) / 255.\n",
    "        print(\"read_data:\" + fname)\n",
    "        code = fname.split(os.sep)[1][9:16]\n",
    "        p = fname.split(os.sep)[1][17] == '1'\n",
    "        yield im, code_to_vec(p, code)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def unzip(b):\n",
    "    xs, ys = zip(*b)\n",
    "    xs = numpy.array(xs)\n",
    "    ys = numpy.array(ys)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def batch(it, batch_size):\n",
    "    out = []\n",
    "    for x in it:\n",
    "        out.append(x)\n",
    "        if len(out) == batch_size:\n",
    "            yield out\n",
    "            out = []\n",
    "    if out:\n",
    "        yield out\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def mpgen(f):\n",
    "    def main(q, args, kwargs):\n",
    "        try:\n",
    "            for item in f(*args, **kwargs):\n",
    "                q.put(item)\n",
    "        finally:\n",
    "            q.close()\n",
    "\n",
    "    @functools.wraps(f)\n",
    "    def wrapped(*args, **kwargs):\n",
    "        q = multiprocessing.Queue(3)\n",
    "        proc = multiprocessing.Process(target=main,\n",
    "                                       args=(q, args, kwargs))\n",
    "        proc.start()\n",
    "        try:\n",
    "            while True:\n",
    "                item = q.get()\n",
    "                yield item\n",
    "        finally:\n",
    "            proc.terminate()\n",
    "            proc.join()\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# @mpgen\n",
    "def read_batches(batch_size):\n",
    "    g = mygenerator.generate_ims()\n",
    "\n",
    "    def gen_vecs():\n",
    "        for im, c, p in itertools.islice(g, batch_size):\n",
    "            yield im, code_to_vec(p, c)\n",
    "\n",
    "    while True:\n",
    "        yield unzip(gen_vecs())\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def get_loss(y, y_):\n",
    "    # Calculate the loss from digits being incorrect.  Don't count loss from\n",
    "    # digits that are in non-present plates.\n",
    "    digits_loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=tf.reshape(y[:, 1:],\n",
    "                          [-1, len(common.CHARS)]),\n",
    "        labels=tf.reshape(y_[:, 1:],\n",
    "                          [-1, len(common.CHARS)]))\n",
    "    digits_loss = tf.reshape(digits_loss, [-1, 7])\n",
    "    digits_loss = tf.reduce_sum(digits_loss, 1)\n",
    "    digits_loss *= (y_[:, 0] != 0)\n",
    "    digits_loss = tf.reduce_sum(digits_loss)\n",
    "\n",
    "    # Calculate the loss from presence indicator being wrong.\n",
    "    presence_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=y_[:, :1], logits=y[:, :1])\n",
    "    presence_loss = 7 * tf.reduce_sum(presence_loss)\n",
    "\n",
    "    return digits_loss, presence_loss, digits_loss + presence_loss\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def train(learn_rate, report_steps, batch_size, initial_weights=None):\n",
    "    \"\"\"\n",
    "    Train the network.\n",
    "    The function operates interactively: Progress is reported on stdout, and\n",
    "    training ceases upon `KeyboardInterrupt` at which point the learned weights\n",
    "    are saved to `weights.npz`, and also returned.\n",
    "    :param learn_rate:\n",
    "        Learning rate to use.\n",
    "    :param report_steps:\n",
    "        Every `report_steps` batches a progress report is printed.\n",
    "    :param batch_size:\n",
    "        The size of the batches used for training.\n",
    "    :param initial_weights:\n",
    "        (Optional.) Weights to initialize the network with.\n",
    "    :return:\n",
    "        The learned network weights.\n",
    "    \"\"\"\n",
    "    x, y, params = model.get_training_model()\n",
    "\n",
    "    y_ = tf.placeholder(tf.float32, [None, 7 * len(common.CHARS) + 1])\n",
    "\n",
    "    digits_loss, presence_loss, loss = get_loss(y, y_)\n",
    "    train_step = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "\n",
    "    best = tf.argmax(tf.reshape(y[:, 1:], [-1, 7, len(common.CHARS)]), 2)\n",
    "    correct = tf.argmax(tf.reshape(y_[:, 1:], [-1, 7, len(common.CHARS)]), 2)\n",
    "\n",
    "    if initial_weights is not None:\n",
    "        assert len(params) == len(initial_weights)\n",
    "        assign_ops = [w.assign(v) for w, v in zip(params, initial_weights)]\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    def vec_to_plate(v):\n",
    "        return \"\".join(common.CHARS[i] for i in v)\n",
    "\n",
    "    def do_report():\n",
    "        r = sess.run([best,\n",
    "                      correct,\n",
    "                      tf.greater(y[:, 0], 0),\n",
    "                      y_[:, 0],\n",
    "                      digits_loss,\n",
    "                      presence_loss,\n",
    "                      loss],\n",
    "                     feed_dict={x: test_xs, y_: test_ys})\n",
    "        num_correct = numpy.sum(\n",
    "            numpy.logical_or(\n",
    "                numpy.all(r[0] == r[1], axis=1),\n",
    "                numpy.logical_and(r[2] < 0.5,\n",
    "                                  r[3] < 0.5)))\n",
    "        r_short = (r[0][:batch_size], r[1][:batch_size], r[2][:batch_size], r[3][:batch_size])\n",
    "        for b, c, pb, pc in zip(*r_short):\n",
    "            print(\"{} {} <-> {} {}\".format(vec_to_plate(c), pc,\n",
    "                                           vec_to_plate(b), float(pb)))\n",
    "        num_p_correct = numpy.sum(r[2] == r[3])\n",
    "\n",
    "        print(\"batch {:3d} correct: {:2.02f}% presence: {:02.02f}% \".format(\n",
    "            batch_idx, 100. * num_correct / (len(r[0])), 100. * num_p_correct / len(r[2])))\n",
    "        print(\"loss: {} (digits: {}, presence: {})\".format(r[6], r[4], r[5]))\n",
    "        print(\"|{}|\".format(\n",
    "            \"\".join(\"X \"[numpy.array_equal(b, c) or (not pb and not pc)] for b, c, pb, pc in zip(*r_short))))\n",
    "\n",
    "    def do_batch():\n",
    "        sess.run(train_step,\n",
    "                 feed_dict={x: batch_xs, y_: batch_ys})\n",
    "        if batch_idx % report_steps == 0:\n",
    "            do_report()\n",
    "\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.90)  ## OOM:0.6x\n",
    "    gpu_options = None\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        sess.run(init)\n",
    "        if initial_weights is not None:\n",
    "            sess.run(assign_ops)\n",
    "\n",
    "        test_xs, test_ys = unzip(list(read_data(\"test/*.png\"))[:batch_size])\n",
    "\n",
    "        try:\n",
    "            last_batch_idx = 0\n",
    "            last_batch_time = time.time()\n",
    "            batch_iter = enumerate(read_batches(batch_size))\n",
    "            for batch_idx, (batch_xs, batch_ys) in batch_iter:\n",
    "                do_batch()\n",
    "                if batch_idx % report_steps == 0:\n",
    "                    batch_time = time.time()\n",
    "                    if last_batch_idx != batch_idx:\n",
    "                        time_for_batches = (60 * (last_batch_time - batch_time) / (last_batch_idx - batch_idx))\n",
    "                        print(\"time for 60 batches {}\".format(time_for_batches))\n",
    "                        print(\"now: \", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "                        last_batch_idx = batch_idx\n",
    "                        last_batch_time = batch_time\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            last_weights = [p.eval() for p in params]\n",
    "            numpy.savez(\"CPUweights.npz\", *last_weights)\n",
    "            return last_weights\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "print(\"Train start! \", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "\n",
    "weights_fname = \"CPUweights.npz\"\n",
    "if weights_fname in os.listdir(os.getcwd()):\n",
    "    f = numpy.load(weights_fname)\n",
    "    initial_weights = [f[n] for n in sorted(f.files,\n",
    "                                            key=lambda s: int(s[4:]))]\n",
    "else:\n",
    "    initial_weights = None\n",
    "\n",
    "train(learn_rate=0.001,\n",
    "      report_steps=20,\n",
    "      batch_size=35,\n",
    "      initial_weights=initial_weights)\n",
    "\n",
    "print(\"Train end! \", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salida del entramiento de train.py\n",
    "\n",
    "\n",
    "now:  2018-10-01 09:02:18<br>\n",
    "PG627CN 1.0 <-> PG627CN 1.0<br>\n",
    "LS152IG 0.0 <-> LS152IG 0.0<br>\n",
    "VG956EH 1.0 <-> VG956EH 1.0<br>\n",
    "HC855DA 0.0 <-> HC855DA 0.0<br>\n",
    "CZ827ZZ 0.0 <-> CZ827ZZ 0.0<br>\n",
    "PS378BQ 0.0 <-> IB378BQ 0.0<br>\n",
    "JH382II 1.0 <-> JH382II 1.0<br>\n",
    "MK637UY 0.0 <-> MK637UY 0.0<br>\n",
    "RD107HY 0.0 <-> RD107UY 0.0<br>\n",
    "BG980TJ 0.0 <-> BG980TJ 0.0<br>\n",
    "HQ496RR 0.0 <-> HQ496RT 0.0<br>\n",
    "OS986CQ 1.0 <-> OS986CQ 1.0<br>\n",
    "HM255YH 0.0 <-> HM255YH 0.0<br>\n",
    "BI974TT 0.0 <-> BI974TT 0.0<br>\n",
    "XH246AQ 1.0 <-> XH246AQ 1.0<br>\n",
    "KW018OM 1.0 <-> KW018OM 1.0<br>\n",
    "ET096EC 0.0 <-> ET096EC 0.0<br>\n",
    "FE378QO 0.0 <-> FE378QO 0.0<br>\n",
    "SN532NS 0.0 <-> SN532NS 0.0<br>\n",
    "JH037NW 1.0 <-> JH037NW 1.0<br>\n",
    "YG310LD 1.0 <-> YG310LD 1.0<br>\n",
    "VB777RQ 0.0 <-> VB777RQ 0.0<br>\n",
    "QQ575PR 1.0 <-> QQ575PR 1.0<br>\n",
    "GJ329AT 1.0 <-> GJ329AT 1.0<br>\n",
    "QC952QR 0.0 <-> QC952QR 0.0<br>\n",
    "FH656MC 1.0 <-> FH656MC 1.0<br>\n",
    "CZ749ZO 0.0 <-> CZ749ZO 0.0<br>\n",
    "ML037ZN 0.0 <-> MK037ZN 0.0<br>\n",
    "XV476RF 0.0 <-> XV476RF 0.0<br>\n",
    "IB777QU 0.0 <-> IB777YT 0.0<br>\n",
    "QL889AZ 0.0 <-> SL889AZ 0.0<br>\n",
    "MU884OK 1.0 <-> MU884OK 1.0<br>\n",
    "EH857YR 0.0 <-> EH857YV 0.0<br>\n",
    "RB756KU 1.0 <-> RB756KU 1.0<br>\n",
    "CX335IR 1.0 <-> CX335IR 1.0<br>\n",
    "batch 101640 correct: 100.00% presence: 100.00% <br>\n",
    "loss: 62.377784729003906 (digits: 53.1773681640625, presence: 9.200414657592773)<br>\n",
    "|                                   |<br>\n",
    "time for 60 batches 149.95138335227966<br>\n",
    "now:  2018-10-01 09:03:08<br>\n",
    "Train end!  2018-10-01 09:04:03<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesando la salida\n",
    "\n",
    "La red para detectar difiere de la utilizada en el entrenamiento en que las dos últimas capas son convolucionales en lugar de fully conected, y la imagen de entrada puede ser de cualquier tamaño en lugar de 128x64. La idea es que toda la imagen en una escala particular puede alimentarse a esta red, lo que produce una imagen con valores de probabilidad de presencia / carácter en cada \"píxel\". La idea aquí es que las ventanas adyacentes compartirán muchas características convolucionales, por lo que al colocarlas en la misma red se evita calcular las mismas características varias veces.\n",
    "\n",
    "\n",
    "Para hacer frente a los duplicados obvios, aplicamos una forma de supresión no máxima a la salida:\n",
    "\n",
    "La técnica utilizada aquí primero agrupa los rectángulos en rectángulos superpuestos, y para cada salida de grupo:\n",
    "* La intersección de todos los cuadros delimitadores.\n",
    "* La patente correspondiente con el rectangulo en el grupo que tenía la mayor probabilidad de estar presente.\n",
    "\n",
    "La salida final termina siendo un solo rectangulo con la probabilidad de la presencia y los caracteres correspondientes:\n",
    "\n",
    "<img src=\"ejemploReconocer.png\">\n",
    "\n",
    "#### detect.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "import common\n",
    "import model\n",
    "\n",
    "\n",
    "def make_scaled_ims(im, min_shape):\n",
    "    ratio = 1. / 2 ** 0.5\n",
    "    shape = (im.shape[0] / ratio, im.shape[1] / ratio)\n",
    "\n",
    "    while True:\n",
    "        shape = (int(shape[0] * ratio), int(shape[1] * ratio))\n",
    "        if shape[0] < min_shape[0] or shape[1] < min_shape[1]:\n",
    "            break\n",
    "        yield cv2.resize(im, (shape[1], shape[0]))\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "def detect(im, param_vals):\n",
    "    \"\"\"\n",
    "    Detect all bounding boxes of number plates in an image.\n",
    "    :param im:\n",
    "        Image to detect number plates in.\n",
    "    :param param_vals:\n",
    "        Model parameters to use. These are the parameters output by the `train`\n",
    "        module.\n",
    "    :returns:\n",
    "        Iterable of `bbox_tl, bbox_br, letter_probs`, defining the bounding box\n",
    "        top-left and bottom-right corners respectively, and a 7,36 matrix\n",
    "        giving the probability distributions of each letter.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to various scales.\n",
    "    scaled_ims = list(make_scaled_ims(im, model.WINDOW_SHAPE))\n",
    "\n",
    "    # Load the model which detects number plates over a sliding window.\n",
    "    x, y, params = model.get_detect_model()\n",
    "\n",
    "    # Execute the model at each scale.\n",
    "    with tf.Session(config=tf.ConfigProto()) as sess:\n",
    "        y_vals = []\n",
    "        \n",
    "        for scaled_im in scaled_ims:\n",
    "            feed_dict = {x: numpy.stack([scaled_im])}\n",
    "            feed_dict.update(dict(zip(params, param_vals)))\n",
    "            y_vals.append(sess.run(y, feed_dict=feed_dict))\n",
    "            plt.imshow(scaled_im)\n",
    "            plt.show()\n",
    "    writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "    # Interpret the results in terms of bounding boxes in the input image.\n",
    "    # Do this by identifying windows (at all scales) where the model predicts a\n",
    "    # number plate has a greater than 50% probability of appearing.\n",
    "    #\n",
    "    # To obtain pixel coordinates, the window coordinates are scaled according\n",
    "    # to the stride size, and pixel coordinates.\n",
    "    count_detect = 0\n",
    "    for i, (scaled_im, y_val) in enumerate(zip(scaled_ims, y_vals)):\n",
    "        for window_coords in numpy.argwhere(y_val[0, :, :, 0] >\n",
    "                                            -math.log(1. / 0.99 - 1)):\n",
    "            letter_probs = (y_val[0,\n",
    "                            window_coords[0],\n",
    "                            window_coords[1], 1:].reshape(\n",
    "                7, len(common.CHARS)))\n",
    "            letter_probs = common.softmax(letter_probs)\n",
    "\n",
    "            img_scale = float(im.shape[0]) / scaled_im.shape[0]\n",
    "\n",
    "            bbox_tl = window_coords * (8, 4) * img_scale\n",
    "            bbox_size = numpy.array(model.WINDOW_SHAPE) * img_scale\n",
    "\n",
    "            present_prob = common.sigmoid(\n",
    "                y_val[0, window_coords[0], window_coords[1], 0])\n",
    "            count_detect += 1\n",
    "            yield bbox_tl, bbox_tl + bbox_size, present_prob, letter_probs\n",
    "            print(\"count detect:\", count_detect)\n",
    "            print(\"show return window: \", bbox_tl, \"return windows box: \", bbox_tl + bbox_size)\n",
    "            print(\"present: \", present_prob)\n",
    "            print(\"letter: \", letter_probs_to_code(letter_probs))\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "\n",
    "def _overlaps(match1, match2):\n",
    "    bbox_tl1, bbox_br1, _, _ = match1\n",
    "    bbox_tl2, bbox_br2, _, _ = match2\n",
    "    return (bbox_br1[0] > bbox_tl2[0] and\n",
    "            bbox_br2[0] > bbox_tl1[0] and\n",
    "            bbox_br1[1] > bbox_tl2[1] and\n",
    "            bbox_br2[1] > bbox_tl1[1])\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "\n",
    "def _group_overlapping_rectangles(matches):\n",
    "    matches = list(matches)\n",
    "    num_groups = 0\n",
    "    match_to_group = {}\n",
    "    for idx1 in range(len(matches)):\n",
    "        for idx2 in range(idx1):\n",
    "            if _overlaps(matches[idx1], matches[idx2]):\n",
    "                match_to_group[idx1] = match_to_group[idx2]\n",
    "                break\n",
    "        else:\n",
    "            match_to_group[idx1] = num_groups\n",
    "            num_groups += 1\n",
    "\n",
    "    groups = collections.defaultdict(list)\n",
    "    for idx, group in match_to_group.items():\n",
    "        groups[group].append(matches[idx])\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "\n",
    "def post_process(matches):\n",
    "    \"\"\"\n",
    "    Use non-maximum suppression on the output of `detect` to filter.\n",
    "    Take an iterable of matches as returned by `detect` and merge duplicates.\n",
    "    Merging consists of two steps:\n",
    "      - Finding sets of overlapping rectangles.\n",
    "      - Finding the intersection of those sets, along with the code\n",
    "        corresponding with the rectangle with the highest presence parameter.\n",
    "    \"\"\"\n",
    "    groups = _group_overlapping_rectangles(matches)\n",
    "\n",
    "    for group_matches in groups.values():\n",
    "        mins = numpy.stack(numpy.array(m[0]) for m in group_matches)\n",
    "        maxs = numpy.stack(numpy.array(m[1]) for m in group_matches)\n",
    "        present_probs = numpy.array([m[2] for m in group_matches])\n",
    "        letter_probs = numpy.stack(m[3] for m in group_matches)\n",
    "\n",
    "        yield (numpy.max(mins, axis=0).flatten(),\n",
    "               numpy.min(maxs, axis=0).flatten(),\n",
    "               numpy.max(present_probs),\n",
    "               letter_probs[numpy.argmax(present_probs)])\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "def letter_probs_to_code(letter_probs):\n",
    "    return \"\".join(common.CHARS[i] for i in numpy.argmax(letter_probs, axis=1))\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "def detect_plate(file_in, weight, file_out):\n",
    "    print(\"detect start! \", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    im = cv2.imread(file_in)\n",
    "\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY) / 255.\n",
    "\n",
    "    plt.imshow(im_gray)\n",
    "    plt.show()\n",
    "\n",
    "    f = numpy.load(weight)\n",
    "\n",
    "    for ii in numpy.load(weight):\n",
    "        if type(f[ii]) != numpy.ndarray:\n",
    "            f.files.pop(f.files.index(ii))\n",
    "\n",
    "    param_vals = [f[n] for n in sorted(f.files, key=lambda s: int(s[-1]))]\n",
    "\n",
    "    for pt1, pt2, present_prob, letter_probs in post_process(\n",
    "            detect(im_gray, param_vals)):\n",
    "        pt1 = tuple(reversed(list(map(int, pt1))))\n",
    "        pt2 = tuple(reversed(list(map(int, pt2))))\n",
    "\n",
    "        code = letter_probs_to_code(letter_probs)\n",
    "\n",
    "        color = (0.0, 255.0, 0.0)\n",
    "        cv2.rectangle(im, pt1, pt2, color)\n",
    "\n",
    "        cv2.putText(im,\n",
    "                    code,\n",
    "                    pt1,\n",
    "                    cv2.FONT_HERSHEY_PLAIN,\n",
    "                    1.5,\n",
    "                    (0, 0, 0),\n",
    "                    thickness=5)\n",
    "\n",
    "        cv2.putText(im,\n",
    "                    code,\n",
    "                    pt1,\n",
    "                    cv2.FONT_HERSHEY_PLAIN,\n",
    "                    1.5,\n",
    "                    (255, 255, 255),\n",
    "                    thickness=2)\n",
    "\n",
    "    cv2.imwrite(file_out, im)\n",
    "    print(\"show result:\")\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n",
    "    print(\"detect end\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "detect_plate(\"prueba.jpg\", \"CPUweights_01102018.npz\", \"prueba.png\")\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "detect_plate(\"prueba2.jpeg\", \"CPUweights_01102018.npz\", \"prueba2.png\")\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "detect_plate(\"prueba3.jpeg\", \"CPUweights_01102018.npz\", \"prueba3.png\")\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "detect_plate(\"prueba4.jpeg\", \"CPUweights_01102018.npz\", \"prueba4.png\")\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "detect_plate(\"prueba5.jpeg\", \"CPUweights_01102018.npz\", \"prueba5.png\")\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "detect_plate(\"prueba6.jpg\", \"CPUweights_01102018.npz\", \"prueba6.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusiones\n",
    "\n",
    "Esta bueno saber que esta red puede detectar patentes con un porcentaje de error bajo y sin necesidad de tener un dataset tageado ya que se genera en el entranamiento. Pero presenta los siguientes problemas que tiene son:\n",
    "* Es lento\n",
    "* Soporta solo el formato configurado\n",
    "* Solo detecta la fuente con la que se entrenó\n",
    "\n",
    "Podemos solucionar la lentitud con una GPU pero tendríamos que revisar los otros items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Utilizando la red (Web Service)\n",
    "\n",
    "Para utilizar esta red la disponibilizamos como webservices, recibiendo la url de una imagen a detectar y produciendo una salida json con la probabilidad de presencia y los caracteres detectados.\n",
    "\n",
    "\n",
    "<img src=\"webservices.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
